{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "\n",
    "В данном домашнем задании вам предстоит реализовать СLIP -- self-supervision модель которая выучивает зависимости между картинками и текстов в едином векторном пространстве. Для выполнения этого домашнего задания вам понадобится GPU и несколько дополнительных библиотек. Автор рекомендует делать все исключительно в Kaggle. \n",
    "\n",
    "\n",
    "[Ссылка на датасет](https://www.kaggle.com/datasets/keenwarrior/small-flicker-data-for-image-captioning)\n",
    "\n",
    "[Ссылка на статью](https://openai.com/research/clip)\n",
    "\n",
    "Задания в ноутбуке будут во многом опираться на статью, поэтому рекомендуется ее прочитать перед выполнением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286e7892-d919-4e36-902d-822051fc429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import timm\n",
    "PATH_TO_IMAGES = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (8 баллов)\n",
    "\n",
    "Для начала нам нужно реализовать составляющие модели: Кодировщик картинок, текста и проектор на какое-то маломерное пространство. В папке с заданием есть соответствующие файлы, заполните пропуски в них опираясь на docstring-и.\n",
    "\n",
    "Разбалловка следующая: \n",
    "\n",
    "Правильно реализованные кодировщики: 2 балла.\n",
    "\n",
    "Правильно реализованный проектор: 2 балла.\n",
    "\n",
    "Правильно реализованный класс СLIP: 4 балла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8033dae-8ebf-4638-85fc-648688885d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .CLIPDataset import CLIPDataset\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from .ImageEncoder import ImageEncoder\n",
    "from .ProjectionHead import ProjectionHead\n",
    "from .TextEncoder import TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db376447-d24d-4df7-9904-a4db7c02beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_embedding=2048, text_embedding=768, temp =1.0):\n",
    "        super().__init__()\n",
    "        self.image_encoder = #TODO\n",
    "        self.text_encoder = #TODO\n",
    "        self.image_projections = #TODO\n",
    "        self.text_projections = #TODO\n",
    "        self.temp = temp\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        :batch: dict of images and text\n",
    "        Here is what you should do:\n",
    "        1) extract image and text features from batch\n",
    "        2) project features into projection space (small latent space)\n",
    "        3) compute cosine similarity with temperature this will be your logits\n",
    "        4) compute \"true\" logits (eg. cosine similarity between images and images, text and text)\n",
    "        5) create targets by averaging similarities from step above (do not forget about temperature)\n",
    "        6) compute mean loss (see paper)\n",
    "        7) return loss\n",
    "\n",
    "        Overall: read paper.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO: \n",
    "        pass\n",
    "    \n",
    "\n",
    "def CE(preds, targets):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. (0 Баллов)\n",
    "\n",
    "Здесь вам нужно вписать правильный путь до csv файла на своей машине и запустить код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03ac2b6c-f76e-4e9a-967f-da89e5da0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{PATH_TO_IMAGES}\")\n",
    "    dataframe[\"id\"] = np.array(list(dataframe.index))\n",
    "    max_id = dataframe[\"id\"].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e03c2216-3007-4126-aa39-35aa40c4f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=1,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3812cb05-d764-42e4-8bac-385dbc89e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"CrossEntropyLoss\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "    \n",
    "    def __format__(self, formatspec):\n",
    "        text = f\"{self.name}: {format(self.avg, formatspec)}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6aabd761-efc4-464a-9278-18732667f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", total=len(train_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n",
    "    return loss_meter\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, validation_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    for batch in tqdm(validation_loader, desc=\"Validating\", total=len(validation_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. (2 балла)\n",
    "\n",
    "За вас написан минимальный код для обучения, если он запускается и модель учится, то за этот пункт вы получите 0.5 балла. Чтобы получить полный балл за задание вам нужно будет провести несколько экспериментов и поподбирать гиперпараметры. Можно начать со статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f1de37ff-d443-4238-b15d-3c6ca673d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCH = 10\n",
    "def procedure():\n",
    "    train_df, validation_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    train_loader, _ = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    val_loader, _ = build_loaders(validation_df, tokenizer, mode=\"valid\")\n",
    "    model = CLIP().to(device)\n",
    "    params = [{\"params\": model.image_encoder.parameters()}, \n",
    "              {\"params\" : model.text_encoder.parameters()},\n",
    "              {\"params\" : itertools.chain(model.image_projections.parameters(),\n",
    "                                          model.text_projections.parameters())}]\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=1, factor=0.8)\n",
    "    step=\"epoch\"\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch: {epoch}. Train and Validation in progress...\")\n",
    "        model.train()\n",
    "        train_loss = train(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        val_loss = validate(model, val_loader)\n",
    "        \n",
    "        lr_scheduler.step(val_loss.avg)\n",
    "        print(f\"Epoch: {epoch},\", end=\"\\n\")\n",
    "        print(f\"Train loss: {train_loss:0.3f}\", end=\"\\n\")\n",
    "        print(f\"Validation loss: {val_loss:0.3f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe63a271-1f68-4eb9-b77e-3d5d6b98aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (0 баллов)\n",
    "\n",
    "Просто посмотрим на результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "92b890cd-9136-4be7-95bb-8c19dd8152ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_image_embeddings(valid_df, model):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    valid_loader, _ = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    valid_image_embeddings = []\n",
    "    for batch in tqdm(valid_loader, desc=\"Getting embeddings\", total=len(valid_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        image_features = model.image_encoder(batch[\"image\"].permute(0, 3, 1, 2)).to(device)\n",
    "        image_embeddings = model.image_projections(image_features)\n",
    "        valid_image_embeddings.append(image_embeddings)\n",
    "    return torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "10a70109-c5a3-4677-9a3c-f83ad7c0ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "@torch.inference_mode()\n",
    "def find_match(model, image_embeddings, text, image_filenames, num_examples=4):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    text_encoded = tokenizer([text])\n",
    "    batch = {key : torch.tensor(value).to(device) for key, value in text_encoded.items()}\n",
    "    \n",
    "    text_features = model.text_encoder(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    text_embeddings = model.text_projections(text_features)\n",
    "    \n",
    "    norm_image_embeddings = nn.functional.normalize(image_embeddings, p=2, dim=-1)\n",
    "    norm_text_embeddings = nn.functional.normalize(text_embeddings, p=2, dim=-1)\n",
    "    \n",
    "    similarity = norm_text_embeddings @ norm_image_embeddings.T\n",
    "    \n",
    "    ans, ans_index = torch.topk(similarity.squeeze(0), num_examples * 5)\n",
    "    match = [image_filenames[index] for index in ans_index[::5]]\n",
    "    fig, ax = plt.subplots(int(num_examples/2), int(num_examples/2), figsize= (10, 10))\n",
    "    for m, a in zip(match, ax.flatten()):\n",
    "        image = Image.open(f\"{PATH_TO_IMAGES}\" + f\"/{m}\")\n",
    "        image = image.convert(\"RGB\")\n",
    "        a.imshow(image)\n",
    "        a.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a465dc-4385-4566-a508-564508c66538",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "image_embeddings = get_image_embeddings(valid_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f8875f-f705-45d8-8cef-81b11a4c21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_match(model, image_embeddings, \"dogs\", valid_df[\"image\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Опишите свои результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
